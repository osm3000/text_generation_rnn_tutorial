{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick system overview on RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Recurrent Neural Network (RNN) is a tool to model *dynamic systems* (for example, modeling temporal data).\n",
    "\n",
    "A dynamical system can be modeled with:\n",
    "\n",
    "$h_t = f(h_{t-1}, x_t)$ \n",
    "\n",
    "where $h_t = g(x_0,...,x_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a recurrent neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ![A simple (vanilla) RNN](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg) |\n",
    "|:--:|\n",
    "| A simple (vanilla) RNN |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is:\n",
    "\n",
    "$h_{t+1} = activation(U * x_t + W * h_{t})$ where activation is usually a *tanh* or *relu* function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different modes of usage\n",
    "| ![Modes](http://karpathy.github.io/assets/rnn/diags.jpeg) |\n",
    "|:--:|\n",
    "| (Reused from Karpathy blog) - Each rectangle is a vector and arrows represent functions (e.g. matrix multiply). Input vectors are in red, output vectors are in blue and green vectors hold the RNN's state (more on this soon). From left to right: (1) Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification). (2) Sequence output (e.g. image captioning takes an image and outputs a sentence of words). (3) Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment). (4) Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French). (5) Synced sequence input and output (e.g. video classification where we wish to label each frame of the video). Notice that in every case are no pre-specified constraints on the lengths sequences because the recurrent transformation (green) is fixed and can be applied as many times as we like. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tokens:  48 \n",
      "\n",
      "Tokens:  ['\\n', ' ', '#', '$', \"'\", '&', '*', '-', '/', '.', '1', '0', '3', '2', '5', '4', '7', '6', '9', '8', 'N', '\\\\', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z'] \n",
      "\n",
      "First 100 characters in the training set:\n",
      " pierre N years old will join the board as a nonexecutive director nov. N \n",
      " mr. is chairman of n.v. \n",
      "Len of train data:  4831321\n"
     ]
    }
   ],
   "source": [
    "def read_data(data_path=\"./data/penn/\"):\n",
    "    train_data = None\n",
    "    val_data = None\n",
    "    test_data = None\n",
    "    with open(data_path+\"train.txt\") as fileHandle:\n",
    "        train_data = list(fileHandle.read())\n",
    "#     with open(data_path+\"test.txt\") as fileHandle:\n",
    "#         test_data = list(fileHandle.read())\n",
    "#    with open(data_path+\"valid.txt\") as fileHandle:\n",
    "#        val_data = list(fileHandle.read())\n",
    "    \n",
    "#    unique_letters = list(set(train_data + test_data + val_data))\n",
    "    unique_letters = list(set(train_data))\n",
    "    nb_tokens = len(unique_letters)\n",
    "    return train_data, test_data, val_data, unique_letters, nb_tokens\n",
    "\n",
    "train_data, test_data, val_data, unique_letters, nb_tokens = read_data()\n",
    "print \"# of tokens: \", nb_tokens, \"\\n\"\n",
    "print \"Tokens: \", unique_letters, \"\\n\"\n",
    "\n",
    "print \"First 100 characters in the training set:\\n\", \"\".join(train_data[:100])\n",
    "print \"Len of train data: \", len(train_data)\n",
    "#print \"Len of test data: \", len(test_data)\n",
    "#print \"Len of validation data: \", len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1]\n",
      " [38]\n",
      " [29]\n",
      " [25]\n",
      " [40]\n",
      " [40]\n",
      " [25]\n",
      " [ 1]\n",
      " [20]\n",
      " [ 1]]\n"
     ]
    }
   ],
   "source": [
    "# Convert letters to numbers\n",
    "train_data_int, test_data_int, val_data_int = [], [], []\n",
    "for letter in train_data:\n",
    "    train_data_int.append(unique_letters.index(letter))\n",
    "\n",
    "# for letter in test_data:\n",
    "#     test_data_int.append(unique_letters.index(letter))\n",
    "\n",
    "# for letter in val_data:\n",
    "#    val_data_int.append(unique_letters.index(letter))\n",
    "\n",
    "train_data_int = np.array(train_data_int).reshape(-1, 1)[0:50000]\n",
    "# test_data_int = np.array(test_data_int).reshape(-1, 1)\n",
    "# val_data_int = np.array(val_data_int).reshape(-1, 1)\n",
    "print train_data_int[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 48)\n",
      "[1]\n",
      "[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Convert our data into one-hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas\n",
    "#train_data_real = pandas.get_dummies(train_data)  # These two matrices are important in order to retrieve the actual letters and writers\n",
    "train_data_onehot = OneHotEncoder(n_values=nb_tokens).fit_transform(train_data_int).toarray()\n",
    "#test_data_onehot = OneHotEncoder(n_values=nb_tokens).fit_transform(test_data_int).toarray()\n",
    "#val_data_onehot = OneHotEncoder(n_values=nb_tokens).fit_transform(val_data_int).toarray()\n",
    "print train_data_onehot.shape\n",
    "print train_data_int[0, :]\n",
    "print train_data_onehot[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49900, 100, 48)\n",
      "(49900, 48)\n"
     ]
    }
   ],
   "source": [
    "# Build sequences\n",
    "def sequence_builder(data, seq_len=100, target_len=1, step=1):\n",
    "    sequence = []\n",
    "    target = []\n",
    "    data_len = data.shape[0]\n",
    "    for i in range(0, data_len, step):\n",
    "        if i+seq_len < data_len:\n",
    "            sequence.append(data[i:i+seq_len, :])\n",
    "            target.append(data[i+seq_len, :])\n",
    "    sequence = np.array(sequence)#.reshape(data_len, seq_len)\n",
    "    target = np.array(target)#.reshape(data_len, target_len)\n",
    "    return sequence, target\n",
    "train_data_onehot_seq, train_data_onehot_target = sequence_builder(train_data_onehot, seq_len=100, target_len=1)\n",
    "print train_data_onehot_seq.shape\n",
    "print train_data_onehot_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping numpy into tensors, then variables\n",
    "train_data_onehot_seq = Variable(torch.from_numpy(train_data_onehot_seq).float(), requires_grad=False)\n",
    "train_data_onehot_target = Variable(torch.from_numpy(train_data_onehot_target).float(), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class rnn_network(nn.Module):\n",
    "    def __init__(self, nb_tokens=3, hidden_size=2):\n",
    "        super(rnn_network, self).__init__()\n",
    "        self.nb_tokens = nb_tokens\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.rnn1 = nn.GRUCell(input_size=nb_tokens, hidden_size=hidden_size)\n",
    "        self.rnn2 = nn.GRUCell(input_size=hidden_size, hidden_size=hidden_size)\n",
    "        \n",
    "        self.output_layer = nn.Linear(in_features=hidden_size, out_features=nb_tokens)\n",
    "        self.softmax = nn.Softmax()\n",
    "    def forward(self, data_min_batch):\n",
    "        \"\"\"\n",
    "        The data mini batch is in the form of : mini_batch_size X number of time steps X dimensions\n",
    "        \"\"\"\n",
    "        hidden_state_layer_0 = Variable(torch.zeros(data_min_batch.size(0), self.hidden_size), requires_grad=False)\n",
    "        hidden_state_layer_1 = Variable(torch.zeros(data_min_batch.size(0), self.hidden_size), requires_grad=False)\n",
    "        \n",
    "        timesteps = data_min_batch.size(1)\n",
    "        rnn_output = []\n",
    "        \n",
    "        for timestep in range(timesteps):\n",
    "            hidden_state_layer_0 = self.rnn1(data_min_batch[:, timestep, :], hidden_state_layer_0)\n",
    "            hidden_state_layer_1 = self.rnn2(hidden_state_layer_0, hidden_state_layer_1)\n",
    "            # print \"hidden_state_layer_1: \", hidden_state_layer_1\n",
    "            rnn_output.append(self.softmax(self.output_layer(hidden_state_layer_1)))\n",
    "        \n",
    "        rnn_output = torch.stack(rnn_output, 1)\n",
    "        print \"rnn_output size: \", rnn_output.size()\n",
    "#         final_output = self.softmax(self.output_layer(rnn_output))\n",
    "        \n",
    "        return rnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn_output size:  torch.Size([2, 100, 48])\n"
     ]
    }
   ],
   "source": [
    "learning_model = rnn_network(nb_tokens=nb_tokens, hidden_size=10)\n",
    "output = learning_model.forward(train_data_onehot_seq[0:2, :, :])\n",
    "# print torch.sum(output, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'util' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-b9999f04540e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlearning_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_onehot_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mdata_in_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m \u001b[0;34m=\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_onehot_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_onehot_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'util' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.Adam(learning_model.parameters(), lr=0.001)\n",
    "\n",
    "nb_epochs = 100\n",
    "for epoch in range(nb_epochs):\n",
    "    train_loss_temp = []\n",
    "    learning_model.train()\n",
    "    for batch, i in enumerate(range(0, train_data_onehot_seq.size(0) - 1, batch_size)):\n",
    "        data_in_batch, labels_batch = \\\n",
    "        getbatch(train_data_onehot_seq, train_data_onehot_target, i=i, batch_size=batch_size)\n",
    "        \n",
    "        output = learning_model.forward(data_in_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        _, data_out_batch = torch.max(data_out_batch, 2)\n",
    "        # Calculating the loss manualy is better - I can be certain of how the loss works\n",
    "        loss = 0\n",
    "        for timestep in range(data_out_batch.size(1)):\n",
    "            loss += loss_fn(output[:, timestep, :].contiguous(), data_out_batch[:,\n",
    "                                                                 timestep].contiguous())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp.append(loss.data[0])\n",
    "    print  \"Ground truth - Epoch \" + str(epoch) + \" -- train loss = \" + str(np.mean(train_loss_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
