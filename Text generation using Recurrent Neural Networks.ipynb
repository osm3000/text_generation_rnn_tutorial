{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick system overview on RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Recurrent Neural Network (RNN) is a tool to model *dynamic systems* (for example, modeling temporal data).\n",
    "\n",
    "A dynamical system can be modeled with:\n",
    "\n",
    "$h_t = f(h_{t-1}, x_t)$ \n",
    "\n",
    "where $h_t = g(x_0,...,x_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a recurrent neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ![A simple (vanilla) RNN](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg) |\n",
    "|:--:|\n",
    "| A simple (vanilla) RNN |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is:\n",
    "\n",
    "$h_{t+1} = activation(U * x_t + W * h_{t})$ where activation is usually a *tanh* or *relu* function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different modes of usage\n",
    "| ![Modes](http://karpathy.github.io/assets/rnn/diags.jpeg) |\n",
    "|:--:|\n",
    "| (Reused from Karpathy blog) - Each rectangle is a vector and arrows represent functions (e.g. matrix multiply). Input vectors are in red, output vectors are in blue and green vectors hold the RNN's state (more on this soon). From left to right: (1) Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification). (2) Sequence output (e.g. image captioning takes an image and outputs a sentence of words). (3) Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment). (4) Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French). (5) Synced sequence input and output (e.g. video classification where we wish to label each frame of the video). Notice that in every case are no pre-specified constraints on the lengths sequences because the recurrent transformation (green) is fixed and can be applied as many times as we like. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tokens:  48 \n",
      "\n",
      "Tokens:  ['\\n', ' ', '#', '$', \"'\", '&', '*', '-', '/', '.', '1', '0', '3', '2', '5', '4', '7', '6', '9', '8', 'N', '\\\\', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z'] \n",
      "\n",
      "First 100 characters in the training set:\n",
      " pierre N years old will join the board as a nonexecutive director nov. N \n",
      " mr. is chairman of n.v. \n",
      "Len of train data:  4831321\n"
     ]
    }
   ],
   "source": [
    "def read_data(data_path=\"./data/penn/\"):\n",
    "    train_data = None\n",
    "    val_data = None\n",
    "    test_data = None\n",
    "    with open(data_path+\"train.txt\") as fileHandle:\n",
    "        train_data = list(fileHandle.read())\n",
    "#     with open(data_path+\"test.txt\") as fileHandle:\n",
    "#         test_data = list(fileHandle.read())\n",
    "#    with open(data_path+\"valid.txt\") as fileHandle:\n",
    "#        val_data = list(fileHandle.read())\n",
    "    \n",
    "#    unique_letters = list(set(train_data + test_data + val_data))\n",
    "    unique_letters = list(set(train_data))\n",
    "    nb_tokens = len(unique_letters)\n",
    "    return train_data, test_data, val_data, unique_letters, nb_tokens\n",
    "\n",
    "train_data, test_data, val_data, unique_letters, nb_tokens = read_data()\n",
    "print \"# of tokens: \", nb_tokens, \"\\n\"\n",
    "print \"Tokens: \", unique_letters, \"\\n\"\n",
    "\n",
    "print \"First 100 characters in the training set:\\n\", \"\".join(train_data[:100])\n",
    "print \"Len of train data: \", len(train_data)\n",
    "#print \"Len of test data: \", len(test_data)\n",
    "#print \"Len of validation data: \", len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1]\n",
      " [38]\n",
      " [29]\n",
      " [25]\n",
      " [40]\n",
      " [40]\n",
      " [25]\n",
      " [ 1]\n",
      " [20]\n",
      " [ 1]]\n"
     ]
    }
   ],
   "source": [
    "# Convert letters to numbers\n",
    "train_data_int, test_data_int, val_data_int = [], [], []\n",
    "for letter in train_data:\n",
    "    train_data_int.append(unique_letters.index(letter))\n",
    "\n",
    "# for letter in test_data:\n",
    "#     test_data_int.append(unique_letters.index(letter))\n",
    "\n",
    "# for letter in val_data:\n",
    "#    val_data_int.append(unique_letters.index(letter))\n",
    "\n",
    "train_data_int = np.array(train_data_int).reshape(-1, 1)[0:50000]\n",
    "# test_data_int = np.array(test_data_int).reshape(-1, 1)\n",
    "# val_data_int = np.array(val_data_int).reshape(-1, 1)\n",
    "print train_data_int[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 48)\n",
      "[1]\n",
      "[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Convert our data into one-hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas\n",
    "#train_data_real = pandas.get_dummies(train_data)  # These two matrices are important in order to retrieve the actual letters and writers\n",
    "train_data_onehot = OneHotEncoder(n_values=nb_tokens).fit_transform(train_data_int).toarray()\n",
    "#test_data_onehot = OneHotEncoder(n_values=nb_tokens).fit_transform(test_data_int).toarray()\n",
    "#val_data_onehot = OneHotEncoder(n_values=nb_tokens).fit_transform(val_data_int).toarray()\n",
    "print train_data_onehot.shape\n",
    "print train_data_int[0, :]\n",
    "print train_data_onehot[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49899, 100, 48)\n",
      "(49899, 100, 48)\n"
     ]
    }
   ],
   "source": [
    "# Build sequences\n",
    "def sequence_builder(data, seq_len=100, target_len=1, step=1):\n",
    "    sequence = []\n",
    "    target = []\n",
    "    data_len = data.shape[0]\n",
    "    for i in range(0, data_len, step):\n",
    "        if i+seq_len+1 < data_len:\n",
    "            sequence.append(data[i:i+seq_len, :])\n",
    "            target.append(data[i+1:i+seq_len+1, :])\n",
    "    sequence = np.array(sequence)#.reshape(data_len, seq_len)\n",
    "    target = np.array(target)#.reshape(data_len, target_len)\n",
    "    return sequence, target\n",
    "train_data_onehot_seq, train_data_onehot_target = sequence_builder(train_data_onehot, seq_len=100, target_len=1)\n",
    "print train_data_onehot_seq.shape\n",
    "print train_data_onehot_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping numpy into tensors, then variables\n",
    "train_data_onehot_seq = Variable(torch.from_numpy(train_data_onehot_seq).float(), requires_grad=False)\n",
    "train_data_onehot_target = Variable(torch.from_numpy(train_data_onehot_target).float(), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class rnn_network(nn.Module):\n",
    "    def __init__(self, nb_tokens=3, hidden_size=2):\n",
    "        super(rnn_network, self).__init__()\n",
    "        self.nb_tokens = nb_tokens\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.rnn1 = nn.GRUCell(input_size=nb_tokens, hidden_size=hidden_size)\n",
    "        self.rnn2 = nn.GRUCell(input_size=hidden_size, hidden_size=hidden_size)\n",
    "        \n",
    "        self.output_layer = nn.Linear(in_features=hidden_size, out_features=nb_tokens)\n",
    "        self.softmax = nn.Softmax()\n",
    "    def forward(self, data_min_batch):\n",
    "        \"\"\"\n",
    "        The data mini batch is in the form of : mini_batch_size X number of time steps X dimensions\n",
    "        \"\"\"\n",
    "        hidden_state_layer_0 = Variable(torch.zeros(data_min_batch.size(0), self.hidden_size), requires_grad=False)\n",
    "        hidden_state_layer_1 = Variable(torch.zeros(data_min_batch.size(0), self.hidden_size), requires_grad=False)\n",
    "        \n",
    "        timesteps = data_min_batch.size(1)\n",
    "        rnn_output = []\n",
    "        \n",
    "        for timestep in range(timesteps):\n",
    "            hidden_state_layer_0 = self.rnn1(data_min_batch[:, timestep, :], hidden_state_layer_0)\n",
    "            hidden_state_layer_1 = self.rnn2(hidden_state_layer_0, hidden_state_layer_1)\n",
    "            # print \"hidden_state_layer_1: \", hidden_state_layer_1\n",
    "            rnn_output.append(self.softmax(self.output_layer(hidden_state_layer_1)))\n",
    "        \n",
    "        rnn_output = torch.stack(rnn_output, 1)\n",
    "#         final_output = self.softmax(self.output_layer(rnn_output))\n",
    "        \n",
    "        return rnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size:  torch.Size([2, 100, 48])\n"
     ]
    }
   ],
   "source": [
    "learning_model = rnn_network(nb_tokens=nb_tokens, hidden_size=10)\n",
    "output = learning_model.forward(train_data_onehot_seq[0:2, :, :])\n",
    "print \"output size: \", output.size()\n",
    "# print torch.sum(output, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just to cut batches\n",
    "def getbatch(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Give it any number of arguments\n",
    "    \"\"\"\n",
    "    i = kwargs['i']\n",
    "    batch_size = kwargs['batch_size']\n",
    "    assert len(args) > 0\n",
    "    output_list = []\n",
    "    # min_len = min(batch_size, len(args[0]) - 1 - i)\n",
    "    min_len = min(batch_size, len(args[0]) - i)\n",
    "    for argument in args:\n",
    "        output_list.append(argument[i:i + min_len])\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth - Epoch 0 -- train loss = -15.0442208831\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-2fc56d64e69b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m             loss += loss_fn(output[:, timestep, :].contiguous(), data_out_batch[:,\n\u001b[1;32m     19\u001b[0m                                                                  timestep].contiguous())\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtrain_loss_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/localdata/mohammom-admin/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/localdata/mohammom-admin/anaconda2/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.Adam(learning_model.parameters(), lr=0.001)\n",
    "\n",
    "nb_epochs = 100\n",
    "for epoch in range(nb_epochs):\n",
    "    train_loss_temp = []\n",
    "    learning_model.train()\n",
    "    for batch, i in enumerate(range(0, train_data_onehot_seq.size(0) - 1, batch_size)):\n",
    "        data_in_batch, data_out_batch = \\\n",
    "        getbatch(train_data_onehot_seq, train_data_onehot_target, i=i, batch_size=batch_size)\n",
    "        \n",
    "        output = learning_model.forward(data_in_batch)\n",
    "        optimizer.zero_grad()\n",
    "        _, data_out_batch = torch.max(data_out_batch, dim=2, keepdim=False)\n",
    "        # Calculating the loss manualy is better - I can be certain of how the loss works\n",
    "        loss = 0\n",
    "        for timestep in range(data_out_batch.size(1)):\n",
    "            loss += loss_fn(output[:, timestep, :].contiguous(), data_out_batch[:,\n",
    "                                                                 timestep].contiguous())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_temp.append(loss.data[0])\n",
    "    print  \"Ground truth - Epoch \" + str(epoch) + \" -- train loss = \" + str(np.mean(train_loss_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
